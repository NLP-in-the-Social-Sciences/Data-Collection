{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# importing data if in json format\n",
    "def load_json_data(file: str) -> dict:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file}' not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON data from file '{file}'.\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# get all txt files from Data Dump folder #TODO: move this to a util file\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "def load_txt_data():\n",
    "    data_points = []\n",
    "    if os.path.exists(file_path):\n",
    "        for file in os.listdir(\"Data Dump\"):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(\"Data Dump\", file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    data_points.append(content)\n",
    "        return np.array(data_points)\n",
    "    else: \n",
    "\n",
    "\n",
    "def  get_recent_query_data():\n",
    "    # Define the repository URL and the file path\n",
    "    repo_url = \"https://github.com/username/repo\"\n",
    "    file_path = \"/path/to/local/file.txt\"\n",
    "\n",
    "    # Clone the repository to a temporary directory\n",
    "    temp_dir = \"/tmp/repo\"\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, temp_dir])\n",
    "\n",
    "    # Move the file from the temporary directory to the desired location\n",
    "    subprocess.run([\"mv\", f\"{temp_dir}/file.txt\", file_path])\n",
    "\n",
    "    # Remove the temporary directory\n",
    "    subprocess.run([\"rm\", \"-rf\", temp_dir])\n",
    "\n",
    "\n",
    "def load_txt_data(repo_url: str, file_path: str):\n",
    "    data_points = []\n",
    "    \n",
    "    # Check if the file exists locally\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            data_points.append(content)\n",
    "    else:\n",
    "        # Download the file from the repository\n",
    "        try:\n",
    "            response = requests.get(repo_url)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text\n",
    "                data_points.append(content)\n",
    "                \n",
    "                # Save the downloaded file locally\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(content)\n",
    "            else:\n",
    "                print(f\"Failed to download file from {repo_url}.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading file from {repo_url}: {e}\")\n",
    "    \n",
    "    return np.array(data_points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.3-cp39-cp39-linux_x86_64.whl size=75070 sha256=5f63e7a64ff7cef577f23c2cde7b3acfca43e7280fadca74a2a6603119557a46\n",
      "  Stored in directory: /home/mkelbessa/.cache/pip/wheels/09/a9/54/37478e65995fe712f7da465749da9ddb21db6b1a599d591ac7\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.3\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\n",
      "  Downloading comm-0.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.9 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.9 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /home/mkelbessa/miniconda3/envs/NLP_WORK/lib/python3.9/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, comm, ipywidgets\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Uninstalling comm-0.1.2:\n",
      "      Successfully uninstalled comm-0.1.2\n",
      "Successfully installed comm-0.2.1 ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 widgetsnbextension-4.0.9\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "# install the following if necessary\n",
    "!pip install --use-pep517 annoy\n",
    "!pip install ipywidgets\n",
    "# if not in a virtual environment,\n",
    "#jupyter nbextension enable --py widgetsnbextension\n",
    "# if in a virtual environment,\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "from functions.preprocessing import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "QA_MODEL_NAME = \"multi-qa-distilbert-cos-v1\"\n",
    "\n",
    "\n",
    "# load model \n",
    "def load_model():\n",
    "    model = SentenceTransformer(QA_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "# there seems to be a bug with the model, so we need to reload it in case it all the initial encoding fails\n",
    "model = load_model()\n",
    "\n",
    "def test_encode(model):\n",
    "    sentence = \"\"\"\n",
    "    Amidst the verdant tapestry of the ancient forest, where sunlight dapples the forest floor and the rustling leaves whisper secrets of generations past, a curious menagerie \n",
    "    of creatures, from the tiniest iridescent insects to the towering, majestic oak trees,coexists in a harmonious symphony of life, while the distant echoes of a \n",
    "    bubbling brook weave a soothing melody through the very heart of this enchanting wilderness.\n",
    "    \"\"\"\n",
    "    assert np.all(model.encode(sentences=sentence) != 0)\n",
    "\n",
    "try:\n",
    "    test_encode(model)\n",
    "except Exception as e:\n",
    "    print(\"Failed to encode sentence, reloading model\")\n",
    "    model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of subreddits to concentrate on \n",
    "!wget https://raw.githubusercontent.com/username/repo/main/subreddits.csv -O subreddits.csv\n",
    "\n",
    "subreddits = pd.read_csv(\"subreddits.csv\").dropna()\n",
    "\n",
    "# load query data \n",
    "query_data = load_data(load_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# sentence = \"Amidst the verdant tapestry of the ancient forest, where sunlight dapples the forest floor and the rustling leaves whisper secrets of generations past, a curious menagerie of creatures, from the tiniest iridescent insects to the towering, majestic oak trees, coexists in a harmonious symphony of life, while the distant echoes of a bubbling brook weave a soothing melody through the very heart of this enchanting wilderness.\"\n",
    "\n",
    "# test_encode = model.encode(sentences=sentence)\n",
    "\n",
    "# if not already filtered, load data and clean it \n",
    "narrative_df = pd.read_csv('unfiltered_sample_narratives.csv')\n",
    "\n",
    "# make lowercase \n",
    "narrative_df['selftext'] = narrative_df['selftext'].str.lower()\n",
    "\n",
    "# remove if there are any null values and narratives have less than 50 words\n",
    "narrative_df = narrative_df.dropna()\n",
    "narrative_df = narrative_df[narrative_df['selftext'].str.split().str.len().gt(50)]\n",
    "\n",
    "narrative_df.to_csv(\"filtered_sample_narratives.csv\")\n",
    "\n",
    "\n",
    "\n",
    "oldnarrative_queries = list(json.load(open(\"local_data.json\", encoding=\"utf-8\"))[\"File\"]) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_queries = list(map(lemmatize, oldnarrative_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize narratives  \n",
    "\n",
    "num_processes = 11\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "results = pool.imap(func=lemmatize,  \n",
    "                    iterable= narrative_df['selftext'],\n",
    "                    chunksize= 24370) \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    " \n",
    "narrative_df[\"tokenized_selftext\"] = np.array([result for result in results])\n",
    "narrative_df.to_csv(\"Tokenized_filtered_2018-01.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Search Queries and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load lemmatized narratives\n",
    "tokenized_narratives_df = pd.read_csv(\"D:\\Reddit Dataset\\Submissions\\Decompressed\\csv\\RS_2018-01_lemmatized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df = tokenized_narratives_df[tokenized_narratives_df[\"subreddit\"].isin(subreddits['subreddit'])]\n",
    "tokenized_narratives_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = model.encode(sentences= tokenized_queries,\n",
    "                                convert_to_numpy=True,\n",
    "                                show_progress_bar=True, \n",
    "                                normalize_embeddings=True)\n",
    "\n",
    "np.save('old-narrative-queries_embeddings.npy', query_embeddings)\n",
    "\n",
    "query_search_index = AnnoyIndex(query_embeddings.shape[1], 'manhattan')\n",
    "\n",
    "for index, embed_value in enumerate(query_embeddings):\n",
    "    query_search_index.add_item(index, embed_value)\n",
    "\n",
    "query_search_index.build(n_trees = 20, n_jobs = 3)\n",
    "\n",
    "query_search_index.save('old-narrative-queries_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can either use the CPU or GPU to encode (use gpu if possible as its faster), but don't use both at the same time so as not to do the computation twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding narratives: CPU version\n",
    "narratives_embedding = model.encode(sentences=tokenized_narratives_df[\"lemmatized_selftext\"], \n",
    "                                    convert_to_numpy=True, \n",
    "                                    show_progress_bar=True)\n",
    "\n",
    "np.save('RS_2018-01_narrative_embeddings.npy', narratives_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding narratives: GPU version\n",
    "pool = model.start_multi_process_pool()\n",
    "\n",
    "narratives_embedding = model.encode_multi_process(sentences=tokenized_narratives_df[\"lemmatized_selftext\"], \n",
    "                                                pool=pool)\n",
    "\n",
    "model.stop_multi_process_pool(pool=pool)\n",
    "\n",
    "np.save('RS_2018-01_narrative_embeddings.npy', narratives_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "\n",
    "for index_embedding, embed_value in enumerate(tqdm(narratives_embedding)):\n",
    "    narratives_search_index.add_item(index_embedding, embed_value)\n",
    "\n",
    "narratives_search_index.build(n_trees = 20, n_jobs = -1)\n",
    "\n",
    "narratives_search_index.save(f'RS_2018-01_narrative_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load embeddings and search indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and search indexes for new narratives \n",
    "narratives_embedding = np.load('RS_2018-01_narrative_embeddings.npy')\n",
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "narratives_search_index.load('RS_2018-01_narrative_search_index.ann')\n",
    "\n",
    "# oldnarrative based: load embeddings and search indexes queries\n",
    "query_embeddings = np.load('old-narrative-queries_embeddings.npy')\n",
    "query_search_index = AnnoyIndex(np.array(query_embeddings).shape[1], 'manhattan')\n",
    "query_search_index.load('old-narrative-queries_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Concatenating (for a single query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive nearest neighbors\n",
    "results_list = []\n",
    "\n",
    "for index, query_embedding in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(query_embedding, n=100, include_distances=True)\n",
    "\n",
    "    result_df = pd.DataFrame(data={\n",
    "        'selftext': narrative_df['selftext'][similar_item_ids[0]],\n",
    "        'title': narrative_df['title'][similar_item_ids[0]],\n",
    "        'ids': narrative_df['id'][similar_item_ids[0]],\n",
    "        'distance': similar_item_ids[1]\n",
    "    })\n",
    "\n",
    "    results_list.append(result_df)\n",
    "\n",
    "results = pd.concat(results_list)\n",
    "results = results.drop_duplicates(subset=['selftext'])\n",
    "results = results.sort_values(by=['distance'], ascending=True)\n",
    "\n",
    "results.to_csv(\"results_tokenized_full.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparative Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(id_to_distance_dictionary: dict, threshold: float) -> float:\n",
    "    \"\"\"\n",
    "    Filters for stories which contain intersecting topics specified by the threshold. \n",
    "\n",
    "    :param id_to_distance_dictionary\n",
    "    :param\n",
    "\n",
    "    return: \n",
    "        tuple containing all the ids for relevant narratives and their respective distances\n",
    "    \"\"\"\n",
    "   \n",
    "    filtered_id_distance_array = []\n",
    "\n",
    "    for comparable_key in id_to_distance_dictionary:\n",
    "        # array we are currently trying to filter\n",
    "        comparable_id_array = id_to_distance_dictionary[comparable_key][0]\n",
    "        comparable_distance_array = id_to_distance_dictionary[comparable_key][1]\n",
    "\n",
    "        for comparable_id_index, comparable_id in enumerate(comparable_id_array):\n",
    "            percentage = 0\n",
    "            for tocompare_key in id_to_distance_dictionary:\n",
    "                if tocompare_key != comparable_key:\n",
    "                    tocompare_id_array = id_to_distance_dictionary[tocompare_key][0]\n",
    "\n",
    "                    if tocompare_id_array.__contains__(comparable_id): \n",
    "                        percentage += (1 / (len(id_to_distance_dictionary)-1))\n",
    "\n",
    "            if percentage >= threshold: \n",
    "                filtered_id_distance_array.append((comparable_id, comparable_distance_array[comparable_id_index],))\n",
    "\n",
    "    return filtered_id_distance_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_dict = {}\n",
    "for index, query_embedding in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(\n",
    "                                    query_embedding,\n",
    "                                    n=1000, \n",
    "                                    include_distances=True)\n",
    "\n",
    "    similar_dict[f\"query_{index}\"] = similar_item_ids\n",
    "\n",
    "\n",
    "filterd_arr= compare(similar_dict, threshold= 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, similar_item_ids in enumerate(filterd_arr): \n",
    "\n",
    "    new_row = {\n",
    "        'selftext': tokenized_narratives_df['selftext'][similar_item_ids[0]],\n",
    "        'title': tokenized_narratives_df['title'][similar_item_ids[0]],\n",
    "        'subreddit': tokenized_narratives_df['subreddit'][similar_item_ids[0]],\n",
    "        'permalink': tokenized_narratives_df['permalink'][similar_item_ids[0]],\n",
    "        'ids': tokenized_narratives_df['id'][similar_item_ids[0]],\n",
    "        'distance': similar_item_ids[1]\n",
    "    }\n",
    "\n",
    "    if index != 0:\n",
    "        filtered_results_df.loc[len(filtered_results_df)] = new_row\n",
    "    else: \n",
    "        filtered_results_df = pd.DataFrame([new_row])\n",
    "\n",
    "filtered_results_df.to_csv(\"Filtered_Results_RS_2018-01.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty row to the bottom of the dataframe\n",
    "import pandas as pd \n",
    "results_df = pd.read_csv(\"Filtered_Results_RS_2018-01.csv\")\n",
    "results_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[results_df[\"distance\"] < 20.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.document_writer import docx_writer\n",
    "from paths import HOME\n",
    "\n",
    "docx_writer(num_people=5, dataframe= results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
