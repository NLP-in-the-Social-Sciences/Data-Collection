{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Word extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    # Tag the tokens with their part of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Define a function to convert part of speech tags to WordNet compatible tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Lemmatize the tokens using WordNet\n",
    "    lemmatizer = wordnet.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    # Identify named entities using NLTK's named entity recognition (NER) module\n",
    "    named_entities = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "\n",
    "    # Extract the most common lemmas and named entities\n",
    "    keywords = [token for token, count in Counter(lemmatized_tokens + [chunk[0] for chunk in named_entities if hasattr(chunk, 'label')]).most_common(10)]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download using nltk.download('punkt') if you get an nltk error\n",
    "all_keywords = []\n",
    "for index, story in enumerate(corpus):\n",
    "    keywords = extract_keywords(story[index])\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the stop words for English\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# choose the set of english stopwords\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess the input corpus\n",
    "def preprocess_text(text):\n",
    "    # tokenize a story within a corpus \n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # remove alphanumeric characters and stop words\n",
    "    cleaned_tokens = [token for token in tokens if token.isalpha() and token not in spacy_stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "cleaned_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorize using TfidfVectorizer which combines counting and noralized weighting\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_keywords(tfidf_scores, feature_names, top_n):\n",
    "    sorted_scores = np.argsort(tfidf_scores)[::-1]\n",
    "    top_keywords = [feature_names[i] for i in sorted_scores[:top_n]]\n",
    "    return top_keywords\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_keywords = get_top_keywords(tfidf_scores, feature_names, 10)\n",
    "    print(f\"Document {i+1} top keywords: {top_keywords}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in relevant stories\n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the path to the file containing the stories\n",
    "file_path = 'path/to/file.csv'\n",
    "\n",
    "# Read in the stories from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    stories = [row['story'] for row in csv_reader]\n",
    "\n",
    "# Define a vectorizer that will convert the stories into a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "\n",
    "# Convert the stories into a matrix of TF-IDF features\n",
    "story_matrix = vectorizer.fit_transform(stories)\n",
    "\n",
    "# Loop through the stories and calculate the cosine similarity between each story and the relevant keywords\n",
    "for i, story in enumerate(stories):\n",
    "    # Convert the story into a matrix of TF-IDF features\n",
    "    story_vec = vectorizer.transform([story])\n",
    "\n",
    "    # Calculate the cosine similarity between the story and the relevant keywords\n",
    "    similarity = cosine_similarity(story_vec, story_matrix[:, [vectorizer.vocabulary_.get(word) for word in relevant_keywords]])\n",
    "\n",
    "    # If the similarity is above a certain threshold, print the story\n",
    "   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake_NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"rake_nltk.txt\" to install all the necessary packages** (```pip install -r requirements.txt```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake(min_length = 1)\n",
    "\n",
    "r.extract_keywords_from_text(corpus[0])\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT & KeyphraseVectorizers (best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"KeyBert_req.txt\" to install all the necessary packages** (```pip install -r requirements.txt```).\n",
    "However, before you install anything, make sure to fulfill the requirements below:\n",
    "* Make sure you installed the following [cuda(especially nvcc)](https://nvidia.github.io/cuda-python/install.html), [spacy](https://spacy.io/usage#quickstart), [visual c++ >2017 and the windows SDK for C++](https://visualstudio.microsoft.com/visual-cpp-build-tools/). The links above should lead you to the installation instruction of each of these libraries in case the pip install of the requirements doesn't work. Visual c++ and the Windows SDK for C++ needs to be installed manually. <u><span style=\"background-color: #f70000\">**Make sure to use Python <= 3.9.**</span><u>\n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT\n",
    "from paths import DATA \n",
    "\n",
    "#init model\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "corpus = load_data(DATA)[\"File\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer based\n",
    "keywords = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        vectorizer = KeyphraseCountVectorizer(spacy_pipeline='en_core_web_sm'), #passing vectorizer in, don't use keyphrase_ngram_range\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range\n",
    "keywords_2 = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        keyphrase_ngram_range = (1,3),\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        stop_words ='english', \n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = [q[0] for x in keywords for q in x]\n",
    "keyphrases_2 = [q[0] for x in keywords_2 for q in x]\n",
    "\n",
    "#change to numpy array \n",
    "import numpy as np\n",
    "\n",
    "keyphrases = np.array(keyphrases)\n",
    "keyphrases_2 = np.array(keyphrases_2)\n",
    "\n",
    "#combine the two arrays into 2-d array\n",
    "keyphrase_arr = np.stack((keyphrases, keyphrases_2), axis = 1)\n",
    "\n",
    "#table of keyphrases\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(keyphrase_arr, columns = ['verctorized_keyphrases', 'ngram_range_keyphrases'])\n",
    "df.to_csv('keyphrases.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to install Microsoft Visual C++ 14.0 or greater and Windows SDK for your specific OS. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/. That is in Individual Components > Compilers, Build tools and runtimes > MSVC ......(latest) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the following packages if not already installed\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#keyphrases.csv in shared drive (not public data)\n",
    "df = pd.read_csv('keyphrases.csv')\n",
    "keyphrases = df['ngram_range_keyphrases'].to_numpy()\n",
    "\n",
    "#embed keyphrases\n",
    "corpus_embeddings = embedder.encode(keyphrases, convert_to_tensor=True)\n",
    "\n",
    "#normalization\n",
    "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_arr  = []\n",
    "\n",
    "for n_clusters in range(5, 16): \n",
    "    clustering_model = KMeans(n_clusters=n_clusters)\n",
    "    clustering_model.fit(corpus_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    kmeans_arr.append(np.array(cluster_assignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences = [[[] for _ in range(n_clusters)] for n_clusters in range(5, 16)]\n",
    "\n",
    "for n_clusters, arr in enumerate(kmeans_arr):\n",
    "    for phrase_id, cluster_id in enumerate(arr):\n",
    "        clustered_sentences[n_clusters][cluster_id].append(keyphrases[phrase_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as json\n",
    "import json\n",
    "index = 10\n",
    "with open(f'clustered_sentences_{index}.json', 'w') as f:\n",
    "\n",
    "    json.dump(clustered_sentences[index], f, indent=4, sort_keys=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "clustering_model_AGC= AgglomerativeClustering(n_clusters=None, distance_threshold=1.5)\n",
    "clustering_model_AGC.fit(corpus_embeddings)\n",
    "cluster_assignment_AGC= clustering_model_AGC.labels_\n",
    "cluster_assignment_AGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences_AGC = {}\n",
    "for phrase_id, cluster_id in enumerate(cluster_assignment_AGC):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(keyphrases[phrase_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the following if necessary\n",
    "!pip install umap-learn altair datasets tqdm \n",
    "!pip install --use-pep517 annoy\n",
    "!pip install ipywidgets\n",
    "# if not in a virtual environment,\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# if in a virtual environment,\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate cohere and import libraries\n",
    "# import umap\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import altair as alt\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "# from annoy import AnnoyIndex\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('sample_narratives.csv')\n",
    "\n",
    "narrative_df = pd.DataFrame({'id': df.id, 'narratives':  df['title'] + '. ' + df['selftext']})\n",
    "\n",
    "# make lowercase \n",
    "narrative_df['narratives'] = narrative_df['narratives'].str.lower()\n",
    "\n",
    "# to use old narratives as queries\n",
    "oldnarrative_queries = list(json.load(open(\"local_data.json\", encoding=\"utf-8\"))[\"File\"]) \n",
    "\n",
    "# # to use keywords as queries \n",
    "# queries = list(json.load(open(\"clustered_sentences_10.json\"))) \n",
    "# queries_combined = [\" \".join(q) for q in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.preprocessing import *\n",
    "from multiprocessing import Pool, cpu_count, Manager\n",
    "\n",
    "# lemmatize queries\n",
    "tokenized_queries = list(map(lemmatize, oldnarrative_queries ))\n",
    "\n",
    "\n",
    "# lemmatized new narratives\n",
    "num_processes = cpu_count()\n",
    "  \n",
    "# get the series that contains the narratives\n",
    "narratives_series =narrative_df[\"narratives\"].to_numpy()\n",
    "\n",
    "chunks = []\n",
    "# split narrative_df into equal parts\n",
    "if narratives_series.shape[0] % num_processes != 0:  # if it doesn't equally split leave the rest out and add it to the last subarray later\n",
    "    chunks = np.split(narratives_series[:-(narrative_df.shape[0] % num_processes)], num_processes)\n",
    "    chunks[-1] = np.append(chunks[-1], narratives_series[-(narratives_series.shape[0] % num_processes):])\n",
    "\n",
    "else: \n",
    "    chunks = np.split(narratives_series, num_processes)\n",
    "\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "#create a shared list and a manager to preserve order \n",
    "result_arr = Manager().list()\n",
    "\n",
    "for result in pool.imap(lemmatize_array, tqdm(chunks)):\n",
    "    result_arr.append(result)\n",
    "\n",
    "pool.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# narratives_series =narrative_df[\"narratives\"].to_numpy()\n",
    "# ch = np.split(narratives_series[:-(narrative_df.shape[0] % 20)], 20)\n",
    "# ch[-1] = np.append(ch[-1], narratives_series[-(narratives_series.shape[0] % 20):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\"tokenized_narratives\": result_arr})\n",
    "\n",
    "merged_narrative_df = pd.concat([narrative_df, result_df], axis=1)\n",
    "merged_narrative_df.to_csv(\"Tokenized_narratives.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Search Queries and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build embeddings for queries(the keywords)\n",
    "query_embeddings = model.encode(sentences= oldnarrative_queries,\n",
    "                                convert_to_numpy=True,\n",
    "                                show_progress_bar=True, \n",
    "                                normalize_embeddings=True)\n",
    "\n",
    "# build index \n",
    "query_search_index = AnnoyIndex(query_embeddings.shape[1], 'manhattan')\n",
    "\n",
    "for index, embed_value in enumerate(query_embeddings):\n",
    "    query_search_index.add_item(index, embed_value)\n",
    "\n",
    "query_search_index.build(n_trees = 20, n_jobs = 3)\n",
    "\n",
    "# query: old narrative based\n",
    "query_search_index.save('old-narrative_queries_search-index.ann')\n",
    "np.save('old-narrative_queries_embeddings.npy', query_embeddings)\n",
    "\n",
    "## keywords\n",
    "# query_search_index.save('old-narrative_query_search-index.ann')\n",
    "# np.save('queries_embeddings.npy', query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-parallelized version\n",
    "narratives_embedding = model.encode(sentences=narrative_df['narratives'].to_numpy(), \n",
    "                                    convert_to_numpy=True, \n",
    "                                    show_progress_bar=True, \n",
    "                                    normalize_embeddings=True)\n",
    "\n",
    "np.save('narratives_embeddings_all.npy', narratives_embedding)\n",
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "\n",
    "for index_embedding, embed_value in enumerate(tqdm(narratives_embedding)):\n",
    "    narratives_search_index.add_item(index_embedding, embed_value)\n",
    "\n",
    "narratives_search_index.build(n_trees = 20, n_jobs = -1)\n",
    "narratives_search_index.save(f'narratives_search_index_all.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load embeddings and search indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and search indexes for new narratives \n",
    "narratives_embedding = np.load('narratives_embeddings.npy')\n",
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "narratives_search_index.load('narratives_search_index.ann')\n",
    "\n",
    "# oldnarrative based: load embeddings and search indexes queries\n",
    "query_embeddings = np.load('old-narrative_queries_embeddings.npy')\n",
    "query_search_index = AnnoyIndex(np.array(query_embeddings).shape[1], 'manhattan')\n",
    "query_search_index.load('old-narrative_queries_search-index.ann')\n",
    "\n",
    "# # keyword based: load embeddings and search indexes queries\n",
    "# query_embeddings = np.load('queries_embeddings.npy')\n",
    "# query_search_index = AnnoyIndex(np.array(query_embeddings).shape[1], 'manhattan')\n",
    "# query_search_index.load('query_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive nearest neighbors\n",
    "\n",
    "results: pd.DataFrame\n",
    "\n",
    "for _, qe in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(qe, n=1000, include_distances=True)\n",
    "\n",
    "    if _ == 0:\n",
    "        results = pd.DataFrame(data={'texts': narrative_df['narratives'][similar_item_ids[0]], \n",
    "                            'ids': narrative_df['id'][similar_item_ids[0]],\n",
    "                            'distance': similar_item_ids[1]})\n",
    "    else:\n",
    "        results = pd.concat([results, pd.DataFrame(data={'texts': narrative_df['narratives'][similar_item_ids[0]], \n",
    "                            'ids': narrative_df['id'][similar_item_ids[0]],\n",
    "                            'distance': similar_item_ids[1]})])\n",
    "\n",
    "results = results.drop_duplicates(subset=['texts'])\n",
    "results = results.sort_values(by=['distance'], ascending=False)\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparative Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(id_to_distance_dictionary: dict, threshold: float) -> float:\n",
    "    \"\"\"\n",
    "    Filters for stories which contain intersecting topics specified by the threshold. \n",
    "\n",
    "    :param id_to_distance_dictionary\n",
    "    :param\n",
    "\n",
    "    return: \n",
    "        tuple containing all the ids for relevant narratives and their respective distances\n",
    "    \"\"\"\n",
    "   \n",
    "    final_array = []\n",
    "\n",
    "    for comparable_index, comparable_key in enumerate(id_to_distance_dictionary):\n",
    "        # array we are currently trying to filter\n",
    "        comparable_id_array = id_to_distance_dictionary[comparable_key][0]\n",
    "        \n",
    "        for index, key in enumerate(id_to_distance_dictionary):\n",
    "            if key != comparable_key:\n",
    "                temp_array = []\n",
    "                percentage = 0\n",
    "                id_array = id_to_distance_dictionary[key][0]\n",
    "\n",
    "                for comparable_id in comparable_id_array: \n",
    "                    if id_array.__contains__(comparable_id): \n",
    "                        percentage += (1 / len(id_to_distance_dictionary)) * 100 \n",
    "                    \n",
    "            if percentage > threshold: \n",
    "                final_array.append(id)\n",
    "\n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_dict = {}\n",
    "for index, query_embedding in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(\n",
    "                                    query_embedding,\n",
    "                                    n=1000, \n",
    "                                    include_distances=True)\n",
    "\n",
    "    similar_dict[f\"query_{index}\"] = similar_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(similar_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_arr= compare(similar_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty row to the bottom of the dataframe\n",
    "empty_row = pd.DataFrame({'texts': [''], 'ids': [''], 'distance': [0]})\n",
    "results = pd.concat([results, empty_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index(drop=True, inplace=True)\n",
    "results.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import HOME\n",
    "from functions.document_writer import docx_writer\n",
    "\n",
    "docx_writer(num_people=5, data = results, path = HOME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
