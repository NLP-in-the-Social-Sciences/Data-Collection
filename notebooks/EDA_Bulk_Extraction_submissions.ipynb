{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Word extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    # Tag the tokens with their part of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Define a function to convert part of speech tags to WordNet compatible tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Lemmatize the tokens using WordNet\n",
    "    lemmatizer = wordnet.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    # Identify named entities using NLTK's named entity recognition (NER) module\n",
    "    named_entities = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "\n",
    "    # Extract the most common lemmas and named entities\n",
    "    keywords = [token for token, count in Counter(lemmatized_tokens + [chunk[0] for chunk in named_entities if hasattr(chunk, 'label')]).most_common(10)]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download using nltk.download('punkt') if you get an nltk error\n",
    "all_keywords = []\n",
    "for index, story in enumerate(corpus):\n",
    "    keywords = extract_keywords(story[index])\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the stop words for English\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# choose the set of english stopwords\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess the input corpus\n",
    "def preprocess_text(text):\n",
    "    # tokenize a story within a corpus \n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # remove alphanumeric characters and stop words\n",
    "    cleaned_tokens = [token for token in tokens if token.isalpha() and token not in spacy_stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "cleaned_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorize using TfidfVectorizer which combines counting and noralized weighting\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_keywords(tfidf_scores, feature_names, top_n):\n",
    "    sorted_scores = np.argsort(tfidf_scores)[::-1]\n",
    "    top_keywords = [feature_names[i] for i in sorted_scores[:top_n]]\n",
    "    return top_keywords\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_keywords = get_top_keywords(tfidf_scores, feature_names, 10)\n",
    "    print(f\"Document {i+1} top keywords: {top_keywords}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in relevant stories\n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the path to the file containing the stories\n",
    "file_path = 'path/to/file.csv'\n",
    "\n",
    "# Read in the stories from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    stories = [row['story'] for row in csv_reader]\n",
    "\n",
    "# Define a vectorizer that will convert the stories into a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "\n",
    "# Convert the stories into a matrix of TF-IDF features\n",
    "story_matrix = vectorizer.fit_transform(stories)\n",
    "\n",
    "# Loop through the stories and calculate the cosine similarity between each story and the relevant keywords\n",
    "for i, story in enumerate(stories):\n",
    "    # Convert the story into a matrix of TF-IDF features\n",
    "    story_vec = vectorizer.transform([story])\n",
    "\n",
    "    # Calculate the cosine similarity between the story and the relevant keywords\n",
    "    similarity = cosine_similarity(story_vec, story_matrix[:, [vectorizer.vocabulary_.get(word) for word in relevant_keywords]])\n",
    "\n",
    "    # If the similarity is above a certain threshold, print the story\n",
    "   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake_NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"rake_nltk.txt\" to install all the necessary packages** (```pip install -r requirements.txt```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake(min_length = 1)\n",
    "\n",
    "r.extract_keywords_from_text(corpus[0])\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT & KeyphraseVectorizers (best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"KeyBert_req.txt\" to install all the necessary packages** (```pip install -r requirements.txt```).\n",
    "However, before you install anything, make sure to fulfill the requirements below:\n",
    "* Make sure you installed the following [cuda(especially nvcc)](https://nvidia.github.io/cuda-python/install.html), [spacy](https://spacy.io/usage#quickstart), [visual c++ >2017 and the windows SDK for C++](https://visualstudio.microsoft.com/visual-cpp-build-tools/). The links above should lead you to the installation instruction of each of these libraries in case the pip install of the requirements doesn't work. Visual c++ and the Windows SDK for C++ needs to be installed manually. <u><span style=\"background-color: #f70000\">**Make sure to use Python <= 3.9.**</span><u>\n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT\n",
    "from paths import DATA \n",
    "\n",
    "#init model\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "corpus = load_data(DATA)[\"File\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer based\n",
    "keywords = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        vectorizer = KeyphraseCountVectorizer(spacy_pipeline='en_core_web_sm'), #passing vectorizer in, don't use keyphrase_ngram_range\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range\n",
    "keywords_2 = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        keyphrase_ngram_range = (1,3),\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        stop_words ='english', \n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = [q[0] for x in keywords for q in x]\n",
    "keyphrases_2 = [q[0] for x in keywords_2 for q in x]\n",
    "\n",
    "#change to numpy array \n",
    "import numpy as np\n",
    "\n",
    "keyphrases = np.array(keyphrases)\n",
    "keyphrases_2 = np.array(keyphrases_2)\n",
    "\n",
    "#combine the two arrays into 2-d array\n",
    "keyphrase_arr = np.stack((keyphrases, keyphrases_2), axis = 1)\n",
    "\n",
    "#table of keyphrases\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(keyphrase_arr, columns = ['verctorized_keyphrases', 'ngram_range_keyphrases'])\n",
    "df.to_csv('keyphrases.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (only for keyword based search, else skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the following packages if not already installed\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#keyphrases.csv in shared drive (not public data)\n",
    "df = pd.read_csv('keyphrases.csv')\n",
    "keyphrases = df['ngram_range_keyphrases'].to_numpy()\n",
    "\n",
    "#embed keyphrases\n",
    "corpus_embeddings = embedder.encode(keyphrases, convert_to_tensor=True)\n",
    "\n",
    "#normalization\n",
    "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_arr  = []\n",
    "\n",
    "for n_clusters in range(5, 16): \n",
    "    clustering_model = KMeans(n_clusters=n_clusters)\n",
    "    clustering_model.fit(corpus_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    kmeans_arr.append(np.array(cluster_assignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences = [[[] for _ in range(n_clusters)] for n_clusters in range(5, 16)]\n",
    "\n",
    "for n_clusters, arr in enumerate(kmeans_arr):\n",
    "    for phrase_id, cluster_id in enumerate(arr):\n",
    "        clustered_sentences[n_clusters][cluster_id].append(keyphrases[phrase_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as json\n",
    "import json\n",
    "index = 10\n",
    "with open(f'clustered_sentences_{index}.json', 'w') as f:\n",
    "\n",
    "    json.dump(clustered_sentences[index], f, indent=4, sort_keys=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "clustering_model_AGC= AgglomerativeClustering(n_clusters=None, distance_threshold=1.5)\n",
    "clustering_model_AGC.fit(corpus_embeddings)\n",
    "cluster_assignment_AGC= clustering_model_AGC.labels_\n",
    "cluster_assignment_AGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences_AGC = {}\n",
    "for phrase_id, cluster_id in enumerate(cluster_assignment_AGC):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(keyphrases[phrase_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the following if necessary\n",
    "!pip install umap-learn altair datasets tqdm \n",
    "!pip install --use-pep517 annoy\n",
    "!pip install ipywidgets\n",
    "# if not in a virtual environment,\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# if in a virtual environment,\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nlplab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import altair as alt\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "# from annoy import AnnoyIndex\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from functions.preprocessing import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already filtered, load data and clean it \n",
    "narrative_df = pd.read_csv('unfiltered_sample_narratives.csv')\n",
    "\n",
    "# make lowercase \n",
    "narrative_df['selftext'] = narrative_df['selftext'].str.lower()\n",
    "\n",
    "# remove if there are any null values and narratives have less than 50 words\n",
    "narrative_df = narrative_df.dropna()\n",
    "narrative_df = narrative_df[narrative_df['selftext'].str.split().str.len().gt(50)]\n",
    "\n",
    "narrative_df.to_csv(\"filtered_sample_narratives.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if already filtered, use the filtered (already cleaned) dataset\n",
    "narrative_df = pd.read_csv(\"filtered_sample_narratives.csv\")\n",
    "\n",
    "# to use old narratives as queries\n",
    "oldnarrative_queries = list(json.load(open(\"local_data.json\", encoding=\"utf-8\"))[\"File\"]) \n",
    "\n",
    "# # to use keywords as queries \n",
    "# queries = list(json.load(open(\"clustered_sentences_10.json\"))) \n",
    "# queries_combined = [\" \".join(q) for q in queries]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_queries = list(map(lemmatize, oldnarrative_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize narratives  \n",
    "num_processes = 11\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "results = pool.imap(func=lemmatize,  \n",
    "                    iterable= narrative_df['selftext'] ,\n",
    "                    chunksize= 24370) \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    " \n",
    "narrative_df[\"tokenized_selftext\"] = np.array([result for result in results])\n",
    "narrative_df.to_csv(\"Tokenized_filtered_2018-01.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Search Queries and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load lemmatized narratives\n",
    "tokenized_narratives_df = pd.read_csv(\"Tokenized_filtered_2018-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1047eb873a854d4d9c7dec9b69c8c2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_embeddings = model.encode(sentences= tokenized_queries,\n",
    "                                convert_to_numpy=True,\n",
    "                                show_progress_bar=True, \n",
    "                                normalize_embeddings=True)\n",
    "\n",
    "query_search_index = AnnoyIndex(query_embeddings.shape[1], 'manhattan')\n",
    "\n",
    "for index, embed_value in enumerate(query_embeddings):\n",
    "    query_search_index.add_item(index, embed_value)\n",
    "\n",
    "query_search_index.build(n_trees = 20, n_jobs = 3)\n",
    "\n",
    "\n",
    "query_search_index.save('old-narrative-queries_search_index.ann')\n",
    "np.save('old-narrative-queries_embeddings.npy', query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU version\n",
    "narratives_embedding = model.encode(sentences=tokenized_narratives_df[\"tokenized_selftext\"], \n",
    "                                    convert_to_numpy=True, \n",
    "                                    show_progress_bar=True)\n",
    "\n",
    "\n",
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "\n",
    "for index_embedding, embed_value in enumerate(tqdm(narratives_embedding)):\n",
    "    narratives_search_index.add_item(index_embedding, embed_value)\n",
    "\n",
    "narratives_search_index.build(n_trees = 20, n_jobs = -1)\n",
    "np.save('2018-01_narratives_embeddings.npy', narratives_embedding)\n",
    "narratives_search_index.save(f'2018-01_narratives_search_index.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268070/268070 [00:10<00:00, 26503.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU version\n",
    "\n",
    "pool = model.start_multi_process_pool()\n",
    "\n",
    "narratives_embedding = model.encode_multi_process(sentences=tokenized_narratives_df[\"tokenized_selftext\"], \n",
    "                                                pool=pool)\n",
    " \n",
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "\n",
    "for index_embedding, embed_value in enumerate(tqdm(narratives_embedding)):\n",
    "    narratives_search_index.add_item(index_embedding, embed_value)\n",
    "\n",
    "narratives_search_index.build(n_trees = 20, n_jobs = -1)\n",
    "\n",
    "np.save('2018-01_narratives_embeddings.npy', narratives_embedding)\n",
    "narratives_search_index.save(f'2018-01_narratives_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load embeddings and search indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and search indexes for new narratives \n",
    "narratives_embedding = np.load('2018-01_narratives_embeddings.npy')\n",
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "narratives_search_index.load('2018-01_narratives_search_index.ann')\n",
    "\n",
    "# oldnarrative based: load embeddings and search indexes queries\n",
    "query_embeddings = np.load('old-narrative-queries_embeddings.npy')\n",
    "query_search_index = AnnoyIndex(np.array(query_embeddings).shape[1], 'manhattan')\n",
    "query_search_index.load('old-narrative-queries_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Concatenating (for a single query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive nearest neighbors\n",
    "results_list = []\n",
    "\n",
    "for index, query_embedding in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(query_embedding, n=100, include_distances=True)\n",
    "\n",
    "    result_df = pd.DataFrame(data={\n",
    "        'selftext': narrative_df['selftext'][similar_item_ids[0]],\n",
    "        'title': narrative_df['title'][similar_item_ids[0]],\n",
    "        'ids': narrative_df['id'][similar_item_ids[0]],\n",
    "        'distance': similar_item_ids[1]\n",
    "    })\n",
    "\n",
    "    results_list.append(result_df)\n",
    "\n",
    "results = pd.concat(results_list)\n",
    "results = results.drop_duplicates(subset=['selftext'])\n",
    "results = results.sort_values(by=['distance'], ascending=True)\n",
    "\n",
    "results.to_csv(\"results_tokenized_full.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparative Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(id_to_distance_dictionary: dict, threshold: float) -> float:\n",
    "    \"\"\"\n",
    "    Filters for stories which contain intersecting topics specified by the threshold. \n",
    "\n",
    "    :param id_to_distance_dictionary\n",
    "    :param\n",
    "\n",
    "    return: \n",
    "        tuple containing all the ids for relevant narratives and their respective distances\n",
    "    \"\"\"\n",
    "   \n",
    "    filtered_id_distance_array = []\n",
    "\n",
    "    for comparable_key in id_to_distance_dictionary:\n",
    "        # array we are currently trying to filter\n",
    "        comparable_id_array = id_to_distance_dictionary[comparable_key][0]\n",
    "        comparable_distance_array = id_to_distance_dictionary[comparable_key][1]\n",
    "\n",
    "        for comparable_id_index, comparable_id in enumerate(comparable_id_array):\n",
    "            percentage = 0\n",
    "            for tocompare_key in id_to_distance_dictionary:\n",
    "                if tocompare_key != comparable_key:\n",
    "                    tocompare_id_array = id_to_distance_dictionary[tocompare_key][0]\n",
    "\n",
    "                    if tocompare_id_array.__contains__(comparable_id): \n",
    "                        percentage += (1 / (len(id_to_distance_dictionary)-1))\n",
    "\n",
    "            if percentage >= threshold: \n",
    "                filtered_id_distance_array.append((comparable_id, comparable_distance_array[comparable_id_index],))\n",
    "\n",
    "    return filtered_id_distance_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 21.95it/s]\n"
     ]
    }
   ],
   "source": [
    "similar_dict = {}\n",
    "for index, query_embedding in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(\n",
    "                                    query_embedding,\n",
    "                                    n=1000, \n",
    "                                    include_distances=True)\n",
    "\n",
    "    similar_dict[f\"query_{index}\"] = similar_item_ids\n",
    "\n",
    "\n",
    "filterd_arr= compare(similar_dict, threshold= 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "53\n",
      "58\n",
      "60\n",
      "83\n",
      "86\n",
      "113\n",
      "117\n",
      "147\n",
      "149\n",
      "165\n",
      "167\n",
      "186\n",
      "215\n",
      "250\n",
      "273\n",
      "278\n",
      "299\n",
      "311\n",
      "325\n",
      "362\n",
      "364\n",
      "380\n",
      "387\n",
      "429\n",
      "449\n",
      "462\n",
      "478\n",
      "488\n",
      "490\n",
      "495\n",
      "502\n",
      "513\n",
      "514\n",
      "520\n",
      "525\n",
      "564\n",
      "570\n",
      "575\n",
      "584\n",
      "586\n",
      "587\n",
      "588\n",
      "598\n",
      "617\n",
      "629\n",
      "654\n",
      "656\n",
      "691\n",
      "705\n",
      "730\n",
      "757\n",
      "762\n",
      "775\n",
      "780\n",
      "788\n",
      "790\n",
      "800\n",
      "802\n",
      "815\n",
      "828\n",
      "842\n",
      "845\n",
      "846\n",
      "853\n",
      "861\n",
      "878\n",
      "880\n",
      "887\n",
      "900\n",
      "903\n",
      "920\n",
      "923\n",
      "969\n",
      "977\n",
      "993\n",
      "995\n",
      "1001\n",
      "1008\n",
      "1033\n",
      "1047\n",
      "1060\n",
      "1067\n",
      "1068\n",
      "1073\n",
      "1079\n",
      "1081\n",
      "1093\n",
      "1096\n",
      "1102\n",
      "1137\n",
      "1139\n",
      "1145\n",
      "1165\n",
      "1172\n",
      "1181\n",
      "1191\n",
      "1205\n",
      "1248\n",
      "1249\n",
      "1256\n",
      "1263\n",
      "1292\n",
      "1293\n",
      "1306\n",
      "1308\n",
      "1320\n",
      "1321\n",
      "1343\n",
      "1353\n",
      "1373\n",
      "1374\n",
      "1379\n",
      "1388\n",
      "1431\n",
      "1456\n",
      "1496\n",
      "1515\n",
      "1518\n",
      "1532\n",
      "1569\n",
      "1588\n",
      "1593\n",
      "1595\n",
      "1604\n",
      "1610\n",
      "1618\n",
      "1626\n",
      "1642\n",
      "1681\n",
      "1682\n",
      "1684\n",
      "1687\n",
      "1708\n",
      "1710\n",
      "1724\n",
      "1728\n",
      "1736\n",
      "1778\n",
      "1782\n",
      "1788\n",
      "1791\n",
      "1798\n",
      "1799\n",
      "1833\n",
      "1842\n",
      "1849\n",
      "1851\n",
      "1873\n",
      "1903\n",
      "1912\n",
      "1923\n",
      "1933\n",
      "1936\n",
      "1962\n",
      "1964\n",
      "1972\n",
      "2015\n",
      "2085\n",
      "2088\n",
      "2130\n",
      "2155\n",
      "2158\n",
      "2160\n",
      "2165\n",
      "2176\n",
      "2184\n",
      "2188\n",
      "2193\n",
      "2214\n",
      "2223\n",
      "2230\n",
      "2255\n",
      "2266\n",
      "2268\n",
      "2307\n",
      "2319\n",
      "2327\n",
      "2328\n",
      "2347\n",
      "2359\n",
      "2361\n",
      "2364\n",
      "2369\n",
      "2383\n",
      "2387\n",
      "2388\n",
      "2397\n",
      "2403\n",
      "2409\n",
      "2456\n",
      "2490\n",
      "2520\n",
      "2527\n",
      "2536\n",
      "2543\n",
      "2544\n",
      "2550\n",
      "2554\n",
      "2562\n",
      "2582\n",
      "2592\n",
      "2600\n",
      "2643\n",
      "2685\n",
      "2704\n",
      "2714\n",
      "2732\n",
      "2738\n",
      "2763\n",
      "2814\n",
      "2827\n",
      "2836\n",
      "2855\n",
      "2869\n",
      "2875\n",
      "2881\n",
      "2911\n",
      "2912\n",
      "2915\n",
      "2917\n",
      "2928\n",
      "2945\n",
      "2957\n",
      "2964\n",
      "2965\n",
      "2993\n",
      "2995\n",
      "3008\n",
      "3015\n",
      "3018\n",
      "3021\n",
      "3041\n",
      "3057\n",
      "3062\n",
      "3076\n",
      "3096\n",
      "3140\n",
      "3145\n",
      "3177\n",
      "3200\n",
      "3210\n",
      "3229\n",
      "3236\n",
      "3251\n",
      "3254\n",
      "3255\n",
      "3263\n",
      "3266\n",
      "3282\n",
      "3298\n",
      "3330\n",
      "3370\n",
      "3372\n",
      "3379\n",
      "3389\n",
      "3406\n",
      "3420\n",
      "3426\n",
      "3436\n",
      "3437\n",
      "3467\n",
      "3472\n",
      "3481\n",
      "3490\n",
      "3512\n",
      "3524\n",
      "3525\n",
      "3567\n",
      "3570\n",
      "3571\n",
      "3577\n",
      "3596\n",
      "3604\n",
      "3615\n",
      "3654\n",
      "3672\n",
      "3680\n",
      "3695\n",
      "3698\n",
      "3720\n",
      "3728\n",
      "3736\n",
      "3746\n",
      "3756\n",
      "3761\n",
      "3762\n",
      "3776\n",
      "3785\n",
      "3800\n",
      "3831\n",
      "3833\n",
      "3852\n",
      "3857\n",
      "3870\n",
      "3885\n",
      "3900\n",
      "3909\n",
      "3931\n",
      "3938\n",
      "3943\n",
      "3970\n",
      "3976\n",
      "3996\n",
      "4017\n",
      "4019\n",
      "4032\n",
      "4034\n",
      "4048\n",
      "4071\n",
      "4100\n",
      "4101\n",
      "4103\n",
      "4117\n",
      "4123\n",
      "4127\n",
      "4147\n",
      "4153\n",
      "4158\n",
      "4192\n",
      "4207\n",
      "4238\n",
      "4256\n",
      "4260\n",
      "4269\n",
      "4278\n",
      "4295\n",
      "4296\n",
      "4308\n",
      "4309\n",
      "4332\n",
      "4350\n",
      "4353\n",
      "4381\n",
      "4390\n",
      "4404\n",
      "4428\n",
      "4440\n",
      "4441\n",
      "4445\n",
      "4503\n",
      "4530\n",
      "4539\n",
      "4545\n",
      "4554\n",
      "4571\n",
      "4574\n",
      "4575\n",
      "4582\n",
      "4599\n",
      "4604\n",
      "4605\n",
      "4622\n",
      "4657\n",
      "4666\n",
      "4692\n",
      "4708\n",
      "4722\n",
      "4740\n",
      "4749\n",
      "4757\n",
      "4792\n",
      "4806\n",
      "4816\n",
      "4824\n",
      "4829\n",
      "4841\n",
      "4844\n",
      "4867\n",
      "4868\n",
      "4886\n",
      "4890\n",
      "4893\n",
      "4903\n",
      "4919\n",
      "4927\n",
      "4943\n",
      "4961\n",
      "4977\n",
      "4983\n",
      "4984\n",
      "4988\n",
      "5001\n",
      "5007\n",
      "5018\n",
      "5025\n",
      "5030\n",
      "5048\n",
      "5050\n",
      "5076\n",
      "5090\n",
      "5101\n",
      "5108\n",
      "5121\n",
      "5137\n",
      "5202\n",
      "5208\n",
      "5233\n",
      "5242\n",
      "5255\n",
      "5264\n",
      "5284\n",
      "5297\n",
      "5316\n",
      "5323\n",
      "5331\n",
      "5346\n",
      "5351\n",
      "5371\n",
      "5372\n",
      "5386\n",
      "5411\n",
      "5419\n",
      "5434\n",
      "5451\n",
      "5452\n",
      "5463\n",
      "5467\n",
      "5472\n",
      "5476\n",
      "5498\n",
      "5505\n",
      "5539\n",
      "5544\n",
      "5553\n",
      "5556\n",
      "5594\n",
      "5598\n",
      "5600\n",
      "5606\n",
      "5666\n",
      "5671\n",
      "5684\n",
      "5696\n",
      "5725\n",
      "5730\n",
      "5762\n",
      "5785\n",
      "5792\n",
      "5800\n",
      "5815\n",
      "5816\n",
      "5848\n",
      "5856\n",
      "5864\n",
      "5890\n",
      "5928\n",
      "5946\n",
      "5974\n",
      "5977\n",
      "5978\n",
      "5981\n",
      "5996\n",
      "6007\n",
      "6022\n",
      "6028\n",
      "6042\n",
      "6045\n",
      "6061\n",
      "6065\n",
      "6093\n",
      "6100\n",
      "6134\n",
      "6177\n",
      "6224\n",
      "6227\n",
      "6232\n",
      "6235\n",
      "6248\n",
      "6282\n",
      "6286\n",
      "6318\n",
      "6350\n",
      "6373\n",
      "6401\n",
      "6419\n",
      "6458\n",
      "6483\n",
      "6493\n",
      "6502\n",
      "6528\n",
      "6536\n",
      "6551\n",
      "6559\n",
      "6570\n",
      "6579\n",
      "6580\n",
      "6583\n",
      "6587\n",
      "6592\n",
      "6599\n",
      "6606\n",
      "6634\n",
      "6637\n",
      "6658\n",
      "6666\n",
      "6702\n",
      "6746\n",
      "6761\n",
      "6769\n",
      "6784\n",
      "6794\n",
      "6801\n",
      "6810\n",
      "6817\n",
      "6818\n",
      "6826\n",
      "6841\n",
      "6844\n",
      "6856\n",
      "6871\n",
      "6876\n",
      "6880\n",
      "6882\n",
      "6883\n",
      "6887\n",
      "6901\n",
      "6908\n",
      "6929\n",
      "6986\n",
      "6989\n",
      "7004\n",
      "7018\n",
      "7019\n",
      "7027\n",
      "7048\n",
      "7054\n",
      "7057\n",
      "7071\n",
      "7078\n",
      "7081\n",
      "7084\n",
      "7090\n",
      "7112\n",
      "7120\n",
      "7147\n",
      "7150\n",
      "7168\n",
      "7180\n",
      "7196\n",
      "7207\n",
      "7215\n",
      "7232\n",
      "7237\n",
      "7250\n",
      "7265\n",
      "7274\n",
      "7295\n",
      "7307\n",
      "7318\n",
      "7338\n",
      "7363\n",
      "7374\n",
      "7377\n",
      "7380\n",
      "7416\n",
      "7472\n",
      "7480\n",
      "7495\n",
      "7502\n",
      "7517\n",
      "7519\n",
      "7579\n",
      "7594\n",
      "7599\n",
      "7646\n",
      "7647\n",
      "7655\n",
      "7669\n",
      "7679\n",
      "7687\n",
      "7688\n",
      "7702\n",
      "7704\n",
      "7745\n",
      "7748\n",
      "7756\n",
      "7787\n",
      "7799\n",
      "7805\n",
      "7815\n",
      "7822\n",
      "7829\n",
      "7843\n",
      "7848\n",
      "7883\n",
      "7885\n",
      "7889\n",
      "7891\n",
      "7904\n",
      "7910\n",
      "7916\n",
      "7947\n",
      "7949\n",
      "7959\n",
      "7963\n",
      "7976\n",
      "7979\n",
      "8038\n",
      "8055\n",
      "8057\n",
      "8063\n",
      "8081\n",
      "8090\n",
      "8107\n",
      "8134\n",
      "8165\n",
      "8185\n",
      "8213\n",
      "8223\n",
      "8228\n",
      "8233\n",
      "8264\n",
      "8272\n",
      "8287\n",
      "8299\n",
      "8300\n",
      "8307\n",
      "8315\n",
      "8321\n",
      "8330\n",
      "8339\n",
      "8343\n",
      "8345\n",
      "8355\n",
      "8381\n",
      "8385\n",
      "8391\n",
      "8418\n",
      "8426\n",
      "8463\n",
      "8472\n",
      "8474\n",
      "8482\n",
      "8495\n",
      "8506\n",
      "8507\n",
      "8522\n",
      "8523\n",
      "8553\n",
      "8578\n",
      "8581\n",
      "8586\n",
      "8599\n",
      "8624\n",
      "8636\n",
      "8657\n",
      "8679\n",
      "8692\n",
      "8696\n",
      "8714\n",
      "8720\n",
      "8806\n",
      "8813\n",
      "8828\n",
      "8830\n",
      "8851\n",
      "8852\n",
      "8860\n",
      "8862\n",
      "8867\n",
      "8915\n",
      "8920\n",
      "8928\n",
      "8930\n",
      "8939\n",
      "8941\n",
      "8948\n",
      "8954\n",
      "8958\n",
      "8970\n",
      "9016\n",
      "9021\n",
      "9111\n",
      "9161\n",
      "9165\n",
      "9183\n",
      "9223\n",
      "9224\n",
      "9232\n",
      "9236\n",
      "9263\n",
      "9294\n",
      "9311\n",
      "9322\n",
      "9334\n",
      "9341\n",
      "9357\n",
      "9368\n",
      "9408\n",
      "9431\n",
      "9440\n",
      "9444\n",
      "9456\n",
      "9474\n",
      "9511\n",
      "9513\n",
      "9524\n",
      "9528\n",
      "9539\n",
      "9553\n",
      "9559\n",
      "9561\n",
      "9571\n",
      "9578\n",
      "9590\n",
      "9601\n",
      "9602\n",
      "9610\n",
      "9629\n",
      "9641\n",
      "9659\n",
      "9660\n",
      "9686\n",
      "9690\n",
      "9696\n",
      "9717\n",
      "9727\n",
      "9733\n",
      "9739\n",
      "9774\n",
      "9818\n",
      "9826\n",
      "9833\n",
      "9861\n",
      "9873\n",
      "9884\n",
      "9901\n",
      "9915\n",
      "9947\n",
      "9958\n",
      "9976\n",
      "9983\n",
      "9991\n",
      "10020\n",
      "10035\n",
      "10052\n",
      "10057\n",
      "10059\n",
      "10063\n",
      "10080\n",
      "10092\n",
      "10093\n",
      "10097\n",
      "10101\n",
      "10118\n",
      "10122\n",
      "10130\n",
      "10134\n",
      "10149\n",
      "10156\n",
      "10160\n",
      "10176\n",
      "10180\n",
      "10210\n",
      "10215\n",
      "10224\n",
      "10259\n",
      "10270\n",
      "10286\n",
      "10289\n",
      "10333\n",
      "10335\n",
      "10372\n",
      "10419\n",
      "10434\n",
      "10436\n",
      "10445\n",
      "10461\n",
      "10475\n",
      "10478\n",
      "10491\n",
      "10493\n",
      "10497\n",
      "10508\n",
      "10511\n",
      "10513\n",
      "10529\n",
      "10531\n",
      "10535\n",
      "10548\n",
      "10580\n",
      "10601\n",
      "10617\n",
      "10638\n",
      "10649\n",
      "10662\n",
      "10677\n",
      "10680\n",
      "10707\n",
      "10748\n",
      "10752\n",
      "10771\n",
      "10777\n",
      "10793\n",
      "10803\n",
      "10808\n",
      "10813\n",
      "10821\n",
      "10846\n",
      "10850\n",
      "10876\n",
      "10882\n",
      "10883\n",
      "10909\n",
      "10914\n",
      "10915\n",
      "10937\n",
      "10956\n",
      "10958\n",
      "10962\n",
      "10967\n",
      "10968\n",
      "10970\n",
      "10973\n",
      "10981\n",
      "10991\n",
      "10993\n",
      "11032\n",
      "11045\n",
      "11063\n",
      "11065\n",
      "11076\n",
      "11085\n",
      "11090\n",
      "11094\n",
      "11105\n",
      "11112\n",
      "11128\n",
      "11129\n",
      "11145\n",
      "11159\n",
      "11183\n",
      "11208\n",
      "11210\n",
      "11215\n",
      "11218\n",
      "11232\n",
      "11235\n",
      "11243\n",
      "11249\n",
      "11251\n",
      "11259\n",
      "11268\n",
      "11282\n",
      "11300\n",
      "11310\n",
      "11320\n",
      "11334\n",
      "11347\n",
      "11357\n",
      "11359\n",
      "11368\n",
      "11373\n",
      "11395\n",
      "11410\n",
      "11412\n",
      "11414\n",
      "11452\n",
      "11482\n",
      "11508\n",
      "11524\n",
      "11554\n",
      "11557\n",
      "11569\n",
      "11581\n",
      "11603\n",
      "11605\n",
      "11615\n",
      "11626\n",
      "11632\n",
      "11644\n",
      "11662\n",
      "11672\n",
      "11690\n",
      "11699\n",
      "11700\n",
      "11711\n",
      "11714\n",
      "11748\n",
      "11803\n",
      "11813\n",
      "11863\n",
      "11907\n",
      "11917\n",
      "11923\n",
      "11945\n",
      "11952\n",
      "11962\n",
      "12003\n",
      "12004\n",
      "12005\n",
      "12009\n",
      "12023\n",
      "12028\n",
      "12029\n",
      "12038\n",
      "12040\n",
      "12049\n",
      "12067\n",
      "12109\n",
      "12110\n",
      "12128\n",
      "12144\n",
      "12166\n",
      "12175\n",
      "12241\n",
      "12249\n",
      "12340\n",
      "12344\n",
      "12361\n",
      "12363\n",
      "12369\n",
      "12379\n",
      "12388\n",
      "12410\n",
      "12438\n",
      "12443\n",
      "12470\n",
      "12473\n",
      "12491\n",
      "12497\n",
      "12517\n",
      "12525\n",
      "12564\n",
      "12566\n",
      "12609\n",
      "12615\n",
      "12618\n",
      "12657\n",
      "12668\n",
      "12705\n",
      "12716\n",
      "12723\n",
      "12748\n",
      "12754\n",
      "12757\n",
      "12766\n",
      "12769\n",
      "12778\n",
      "12781\n",
      "12799\n",
      "12804\n",
      "12834\n",
      "12844\n",
      "12859\n",
      "12861\n",
      "12865\n",
      "12874\n",
      "12892\n",
      "12899\n",
      "12922\n",
      "12927\n",
      "12945\n",
      "12962\n",
      "12974\n",
      "12975\n",
      "12977\n",
      "12987\n",
      "13002\n",
      "13006\n",
      "13007\n",
      "13017\n",
      "13028\n",
      "13038\n",
      "13054\n",
      "13060\n",
      "13068\n",
      "13070\n",
      "13102\n",
      "13120\n",
      "13126\n",
      "13128\n",
      "13175\n",
      "13179\n",
      "13186\n",
      "13187\n",
      "13188\n",
      "13195\n",
      "13204\n",
      "13207\n",
      "13211\n",
      "13233\n",
      "13239\n",
      "13253\n",
      "13269\n",
      "13298\n",
      "13309\n",
      "13344\n",
      "13364\n",
      "13366\n",
      "13371\n",
      "13379\n",
      "13383\n",
      "13415\n",
      "13432\n",
      "13435\n",
      "13437\n",
      "13495\n",
      "13510\n"
     ]
    }
   ],
   "source": [
    "for x in similar_dict['query_0'][0]: \n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, similar_item_ids in enumerate(filterd_arr): \n",
    "\n",
    "    new_row = {\n",
    "        'selftext': narrative_df['selftext'][similar_item_ids[0]],\n",
    "        'title': narrative_df['title'][similar_item_ids[0]],\n",
    "        'ids': narrative_df['id'][similar_item_ids[0]],\n",
    "        'distance': similar_item_ids[1]\n",
    "    }\n",
    "    \n",
    "    if index != 0:\n",
    "        filtered_results_df.append(new_row, ignore_index=True)\n",
    "    else: \n",
    "        filtered_results_df = pd.DataFrame([new_row])\n",
    "\n",
    "filtered_results_df.to_csv(\"Filtered_Results_2018-01.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty row to the bottom of the dataframe\n",
    "import pandas as pd \n",
    "results_df = pd.read_csv(\"Filtered_Results_2018-01.csv.csv\")\n",
    "results_df = results_df[1:]\n",
    "results_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.document_writer import docx_writer\n",
    "from paths import HOME\n",
    "\n",
    "docx_writer(num_people=5, data = results_df, paths=HOME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
