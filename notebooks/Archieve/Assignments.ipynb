{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import Library and Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"og5lNcywUZ0M"},"outputs":[],"source":["import pandas as pd \n","import pathlib\n","import os\n","from docx import Document\n","from docx.text.parfmt import ParagraphFormat \n","from docx.shared import Inches, RGBColor\n","\n","NUMBER_OF_PEOPLE = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cvq12iyUZ0Q","outputId":"4c1a2d05-cc65-44e2-ccdc-300e5c5cc4e6"},"outputs":[],"source":["#import data\n","path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Data Set\\Filtered and SQL\\Filtered MDF.csv\"\n","df = pd.read_csv(path)\n","\n","df.columns\n","# df = df.sample(frac=1).reset_index(drop=True)\n","# df[\"Number_of_Paragraphs\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.drop_duplicates(subset='url').shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["### Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ya9j-W2XUZ0P"},"outputs":[],"source":["### Source: https://github.com/python-openxml/python-docx/issues/74#issuecomment-261169410\n","\n","import docx\n","def add_hyperlink(paragraph, url, text, color, underline):\n","    \"\"\"\n","    A function that places a hyperlink within a paragraph object.\n","\n","    :param paragraph: The paragraph we are adding the hyperlink to.\n","    :param url: A string containing the required url\n","    :param text: The text displayed for the url\n","    :return: The hyperlink object\n","    \"\"\"\n","\n","    # This gets access to the document.xml.rels file and gets a new relation id value\n","    part = paragraph.part\n","    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n","\n","    # Create the w:hyperlink tag and add needed values\n","    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n","    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n","\n","    # Create a w:r element\n","    new_run = docx.oxml.shared.OxmlElement('w:r')\n","\n","    # Create a new w:rPr element\n","    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n","\n","    # Add color if it is given\n","    if not color is None:\n","      c = docx.oxml.shared.OxmlElement('w:color')\n","      c.set(docx.oxml.shared.qn('w:val'), color)\n","      rPr.append(c)\n","\n","    # Remove underlining if it is requested\n","    if not underline:\n","      u = docx.oxml.shared.OxmlElement('w:u')\n","      u.set(docx.oxml.shared.qn('w:val'), 'none')\n","      rPr.append(u)\n","\n","    # Join all the xml elements together add add the required text to the w:r element\n","    new_run.append(rPr)\n","    new_run.text = text\n","    hyperlink.append(new_run)\n","\n","    paragraph._p.append(hyperlink)\n","\n","    return hyperlink\n","\n","# # extract links from hyperlink text in  \n","# def extract_links(file_path):\n","#   # Open the .docx file\n","#   doc = docx.Document(file_path)\n","  \n","#   # Initialize an empty list to store the links\n","#   links = []\n","  \n","#   # Iterate through all the paragraphs in the document\n","#   for para in doc.paragraphs:\n","#       # Iterate through all the runs in the paragraph\n","#       for run in para.runs:\n","#           # Check if the run has a hyperlink\n","#           if run.hyperlink:\n","#               # If it does, add the hyperlink to the list of links\n","#               links.append(run.hyperlink.address)\n","  \n","#   # Return the list of links\n","#   return links\n","\n","# def extract_title(file_path):\n","#     # Open the .docx file\n","#     doc = docx.Document(file_path)\n","    \n","#     # Iterate through all the paragraphs in the document\n","#     for para in doc.paragraphs:\n","#         # Get the text of the paragraph\n","#         text = para.text\n","        \n","#         # Split the text into lines\n","#         lines = text.split(\"\\n\")\n","        \n","#         # Iterate through all the lines\n","#         for line in lines:\n","#             # Check if the line starts with \"Title:\"\n","#             if line.startswith(\"Title:\"):\n","#                 # If it does, extract the text after \"Title:\" and return it\n","#                 title = line.split(\"Title:\")[1].strip()\n","#                 return title\n","    \n","#     # If no \"Title:\" is found, return None\n","#     return None\n","\n","def extract_title(text):\n","    # Split the text into lines\n","    lines = text.split(\"\\n\")\n","    \n","    # Iterate through all the lines\n","    for line in lines:\n","        # Check if the line starts with \"Title:\"\n","        if line.startswith(\"Title:\"):\n","            # If it does, extract the text after \"Title:\" and return it\n","            title = line.split(\"Title:\")[1].strip()\n","            return title\n","    \n","    # If no \"Title:\" is found, return None\n","    return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Example usage\n","text = \"Post Url: Link to Post, Subreddit: r/college, Title: Seeking input on my list of potential transfer colleges\"\n","# title = extract_title(text)\n","\n","lines = text.split(\"\\n\")\n","lines\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Creating Assignment Docs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zw7_r9taUZ0R"},"outputs":[],"source":["\"\"\"\n","Note: Remember to rerun this before every rerun of the cell below. This because the document object needs to be renewed for every time this is done. \n","I will definitely make this a function once I finalize everything else. \n","\"\"\"\n","\n","#create document files for each subreddit in \n","docs = []\n","    \n","[docs.append(Document()) for i in range(NUMBER_OF_PEOPLE)] \n","\n","#create dataframe for each person on the team\n","dfs = []\n","\n","#randomize indexes to adjust for the length of =\n","\n","index_num = [x for x in range(0, df.shape[0]+1,df.shape[0]//NUMBER_OF_PEOPLE)]\n","\n","for i in range(0, len(index_num)-1, 1): #could be improved\n","    dfs.append(df.iloc[index_num[i]:index_num[i+1]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FLRqCkTUZ0S"},"outputs":[],"source":["count = 0 \n","for i in range(len(dfs)): \n","\n","    for index in range(len(dfs[i])): \n","        # first line of each narrative which contains the link to the submission and the subreddit\n","        firstLine = docs[count].add_paragraph(\"Post Url: \")\n","        add_hyperlink(firstLine, dfs[i].iloc[index]['url'], 'Link to Post', '00a7ff', False)\n","        firstLine.add_run(', Subreddit: r/'+ dfs[i].iloc[index]['subreddit'])\n","\n","        #paragraph and format\n","        para = docs[count].add_paragraph(dfs[i].iloc[index]['selftext'])\n","        para.paragraph_format.first_line_indent = Inches(0.5)\n","        para.paragraph_format.right_indent = Inches(0.5)\n","        para.keep_together = True\n","        docs[count].add_page_break() \n","\n","    docs[count].save(\"Person_\" + str(count)+ \".docx\")\n","    count += 1"]},{"cell_type":"markdown","metadata":{},"source":["### Create Validated Dataset Doc"]},{"cell_type":"markdown","metadata":{},"source":["* Estephanos: 10 \n","* Labiba: 4 \n","* Anubav: 15(one was a duplicate)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validated_Dataframe = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{},"source":["#### Get data from text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#for estephanos\n","links = []\n","\n","with open(\"text files\\Just links_estephanos.txt\", 'r') as file: \n","    # Iterate through all the lines\n","    for line in file:\n","        links.append(str(line[:-2]) + \"/\") # remove '\\n' while appending\n","\n","for link in links:\n","    validated_Dataframe = pd.concat([validated_Dataframe, df.loc[(df['url'] == link)]])\n","\n","validated_Dataframe = pd.concat([validated_Dataframe, df.loc[(df['url'] == \"https://www.reddit.com/r/college/comments/ir9444/considering_dropping_out_advice/\")]])\n","\n","validated_Dataframe = validated_Dataframe.drop_duplicates(subset=['title'])\n","validated_Dataframe.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#for anubav\n","links_1 = []\n","with open(\"text files\\\\Just links_anubav.txt\", 'r') as file: \n","    # Iterate through all the lines\n","    for line in file: \n","        links_1.append(str(line[:-1])) # remove '\\n' while appending\n","\n","links_1[-1] += \"/\"\n","\n","for link in links_1:\n","    validated_Dataframe = pd.concat([validated_Dataframe, df.loc[(df['url'] == link)]])\n","\n","validated_Dataframe = validated_Dataframe.drop_duplicates(subset=['title'])\n","validated_Dataframe.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for labiba\n","links_2 = []\n","with open(\"text files\\\\Just links_Labiba.txt\", 'r') as file: \n","    # Iterate through all the lines\n","    for line in file: \n","        links_2.append(str(line[:-1])) # remove '\\n' while appending\n","\n","links_2[-1] += \"/\"\n","for link in links_2:\n","    validated_Dataframe = pd.concat([validated_Dataframe, df.loc[(df['url'] == link)]])\n","\n","validated_Dataframe = validated_Dataframe.drop_duplicates(subset=['title'])\n","validated_Dataframe.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for ilyas"]},{"cell_type":"markdown","metadata":{},"source":["#### Add to docx file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from docx import Document\n","from docx.text.parfmt import ParagraphFormat \n","from docx.shared import Inches, RGBColor\n","\n","validated_Dataframe = validated_Dataframe.reset_index(drop=True) # reset index of dataframe \n","\n","validation_document = Document()\n","for index in range(len(validated_Dataframe)): \n","    #heading for each story\n","    firstLine = validation_document.add_paragraph(\"Post Url: \")\n","    add_hyperlink(firstLine, validated_Dataframe.iloc[index]['url'], 'Link to Post', '00a7ff', False)\n","    firstLine.add_run(', Subreddit: r/'+ validated_Dataframe.iloc[index]['subreddit'])\n","    firstLine.add_run(', Title: '+ validated_Dataframe.iloc[index]['title'])\n","\n","    #paragraph and format\n","    para = validation_document.add_paragraph(validated_Dataframe.iloc[index]['selftext'])\n","    para.keep_together = True\n","\n","    validation_document.add_page_break() \n","# path = \"C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\New Narratives\"\n","validation_document.save(\"Validation Document_lack.docx\")"]},{"cell_type":"markdown","metadata":{},"source":["### Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(validated_Dataframe.subreddit.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":0}
