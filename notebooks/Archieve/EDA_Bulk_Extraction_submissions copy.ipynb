{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity Recognition: NLTK Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "syns = wordnet.synsets(\"program\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    # Tag the tokens with their part of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Define a function to convert part of speech tags to WordNet compatible tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Lemmatize the tokens using WordNet\n",
    "    lemmatizer = wordnet.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    # Identify named entities using NLTK's named entity recognition (NER) module\n",
    "    named_entities = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "\n",
    "    # Extract the most common lemmas and named entities\n",
    "    keywords = [token for token, count in Counter(lemmatized_tokens + [chunk[0] for chunk in named_entities if hasattr(chunk, 'label')]).most_common(10)]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download using nltk.download('punkt') if you get an nltk error\n",
    "corpus = load_data(\"new_local_data.json\")\n",
    "\n",
    "all_keywords = []\n",
    "for index, story in enumerate(corpus):\n",
    "    keywords = extract_keywords(story[index])\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the stop words for English\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# choose the set of english stopwords\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess the input corpus\n",
    "def preprocess_text(text):\n",
    "    # tokenize a story within a corpus \n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # remove alphanumeric characters and stop words\n",
    "    cleaned_tokens = [token for token in tokens if token.isalpha() and token not in spacy_stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "cleaned_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorize using TfidfVectorizer which combines counting and noralized weighting\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_keywords(tfidf_scores, feature_names, top_n):\n",
    "    sorted_scores = np.argsort(tfidf_scores)[::-1]\n",
    "    top_keywords = [feature_names[i] for i in sorted_scores[:top_n]]\n",
    "    return top_keywords\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_keywords = get_top_keywords(tfidf_scores, feature_names, 10)\n",
    "    print(f\"Document {i+1} top keywords: {top_keywords}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in relevant stories\n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the path to the file containing the stories\n",
    "file_path = 'path/to/file.csv'\n",
    "\n",
    "# Read in the stories from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    stories = [row['story'] for row in csv_reader]\n",
    "\n",
    "# Define a vectorizer that will convert the stories into a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "\n",
    "# Convert the stories into a matrix of TF-IDF features\n",
    "story_matrix = vectorizer.fit_transform(stories)\n",
    "\n",
    "# Loop through the stories and calculate the cosine similarity between each story and the relevant keywords\n",
    "for i, story in enumerate(stories):\n",
    "    # Convert the story into a matrix of TF-IDF features\n",
    "    story_vec = vectorizer.transform([story])\n",
    "\n",
    "    # Calculate the cosine similarity between the story and the relevant keywords\n",
    "    similarity = cosine_similarity(story_vec, story_matrix[:, [vectorizer.vocabulary_.get(word) for word in relevant_keywords]])\n",
    "\n",
    "    # If the similarity is above a certain threshold, print the story\n",
    "   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake_NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"rake_nltk.txt\" to install all the necessary packages** (```pip install -r requirements.txt```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake(min_length = 1)\n",
    "\n",
    "r.extract_keywords_from_text(corpus[0])\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT & KeyphraseVectorizers (best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"KeyBert_req.txt\" to install all the necessary packages** (```pip install -r requirements.txt```).\n",
    "However, before you install anything, make sure to fulfill the requirements below:\n",
    "* Make sure you installed the following [cuda(especially nvcc)](https://nvidia.github.io/cuda-python/install.html), [spacy](https://spacy.io/usage#quickstart), [visual c++ >2017 and the windows SDK for C++](https://visualstudio.microsoft.com/visual-cpp-build-tools/). The links above should lead you to the installation instruction of each of these libraries in case the pip install of the requirements doesn't work. Visual c++ and the Windows SDK for C++ needs to be installed manually. <u><span style=\"background-color: #f70000\">**Make sure to use Python <= 3.9.**</span><u>\n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT\n",
    "from paths import NEW_DATA\n",
    "\n",
    "#init model\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "corpus = load_data(NEW_DATA)[\"File\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer based\n",
    "keywords = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        vectorizer = KeyphraseCountVectorizer(spacy_pipeline='en_core_web_sm'), #passing vectorizer in, don't use keyphrase_ngram_range\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range\n",
    "keywords_2 = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        keyphrase_ngram_range = (1,3),\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        stop_words ='english', \n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = [q[0] for x in keywords for q in x]\n",
    "keyphrases_2 = [q[0] for x in keywords_2 for q in x]\n",
    "\n",
    "# change to numpy array \n",
    "import numpy as np\n",
    "\n",
    "keyphrases = np.array(keyphrases)\n",
    "keyphrases_2 = np.array(keyphrases_2)\n",
    "\n",
    "# combine the two arrays into 2-d array\n",
    "keyphrase_arr = np.stack((keyphrases, keyphrases_2), axis = 1)\n",
    "\n",
    "#table of keyphrases\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(keyphrase_arr, columns = ['verctorized_keyphrases', 'ngram_range_keyphrases'])\n",
    "df.to_csv('keyphrases.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (only for keyword based search, else skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the following packages if not already installed\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#keyphrases.csv in shared drive (not public data)\n",
    "df = pd.read_csv('keyphrases.csv')\n",
    "keyphrases = df['ngram_range_keyphrases'].to_numpy()\n",
    "\n",
    "#embed keyphrases\n",
    "corpus_embeddings = embedder.encode(keyphrases, convert_to_tensor=True)\n",
    "\n",
    "#normalization\n",
    "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_arr  = []\n",
    "\n",
    "for n_clusters in range(5, 16): \n",
    "    clustering_model = KMeans(n_clusters=n_clusters)\n",
    "    clustering_model.fit(corpus_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    kmeans_arr.append(np.array(cluster_assignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences = [[[] for _ in range(n_clusters)] for n_clusters in range(5, 16)]\n",
    "\n",
    "for n_clusters, arr in enumerate(kmeans_arr):\n",
    "    for phrase_id, cluster_id in enumerate(arr):\n",
    "        clustered_sentences[n_clusters][cluster_id].append(keyphrases[phrase_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as json\n",
    "import json\n",
    "index = 10\n",
    "with open(f'clustered_sentences_{index}.json', 'w') as f:\n",
    "\n",
    "    json.dump(clustered_sentences[index], f, indent=4, sort_keys=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "clustering_model_AGC= AgglomerativeClustering(n_clusters=None, distance_threshold=1.5)\n",
    "clustering_model_AGC.fit(corpus_embeddings)\n",
    "cluster_assignment_AGC= clustering_model_AGC.labels_\n",
    "cluster_assignment_AGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences_AGC = {}\n",
    "for phrase_id, cluster_id in enumerate(cluster_assignment_AGC):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(keyphrases[phrase_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the following if necessary\n",
    "!pip install --use-pep517 annoy\n",
    "!pip install ipywidgets\n",
    "# if not in a virtual environment,\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# if in a virtual environment,\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Get dataset\n",
    "dataset = load_dataset(\"trec\", split=\"train\")\n",
    "# Import into a pandas dataframe, take only the first 1000 rows\n",
    "df = pd.DataFrame(dataset)[:1000]\n",
    "# Preview the data to ensure it has loaded correctly\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import altair as alt\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "from functions.preprocessing import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = pd.read_csv(\"subreddits.csv\").dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the cell below twice!!** (there seems to be a bug here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "# sentence = \"Amidst the verdant tapestry of the ancient forest, where sunlight dapples the forest floor and the rustling leaves whisper secrets of generations past, a curious menagerie of creatures, from the tiniest iridescent insects to the towering, majestic oak trees, coexists in a harmonious symphony of life, while the distant echoes of a bubbling brook weave a soothing melody through the very heart of this enchanting wilderness.\"\n",
    "\n",
    "# test_encode = model.encode(sentences=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already filtered, load data and clean it \n",
    "narrative_df = pd.read_csv('unfiltered_sample_narratives.csv')\n",
    "\n",
    "# make lowercase \n",
    "narrative_df['selftext'] = narrative_df['selftext'].str.lower()\n",
    "\n",
    "# remove if there are any null values and narratives have less than 50 words\n",
    "narrative_df = narrative_df.dropna()\n",
    "narrative_df = narrative_df[narrative_df['selftext'].str.split().str.len().gt(50)]\n",
    "\n",
    "narrative_df.to_csv(\"filtered_sample_narratives.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if already filtered, use the filtered (already cleaned) dataset\n",
    "# narrative_df = pd.read_csv(\"D:\\Reddit Dataset\\Submissions\\Decompressed\\csv\\RS_2018-03_lemmatized.csv\")\n",
    "\n",
    "# to use old narratives as queries\n",
    "oldnarrative_queries = list(json.load(open(\"local_data.json\", encoding=\"utf-8\"))[\"File\"]) \n",
    "\n",
    "# # to use keywords as queries \n",
    "# queries = list(json.load(open(\"clustered_sentences_10.json\"))) \n",
    "# queries_combined = [\" \".join(q) for q in queries]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_queries = list(map(lemmatize, oldnarrative_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize narratives  \n",
    "num_processes = 11\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "results = pool.imap(func=lemmatize,  \n",
    "                    iterable= narrative_df['selftext'] ,\n",
    "                    chunksize= 24370) \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    " \n",
    "narrative_df[\"tokenized_selftext\"] = np.array([result for result in results])\n",
    "narrative_df.to_csv(\"Tokenized_filtered_2018-01.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Search Queries and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load lemmatized narratives\n",
    "tokenized_narratives_df = pd.read_csv(\"D:\\Reddit Dataset\\Submissions\\Decompressed\\csv\\RS_2018-01_lemmatized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df = tokenized_narratives_df[tokenized_narratives_df[\"subreddit\"].isin(subreddits['subreddit'])]\n",
    "tokenized_narratives_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = model.encode(sentences= tokenized_queries,\n",
    "                                convert_to_numpy=True,\n",
    "                                show_progress_bar=True, \n",
    "                                normalize_embeddings=True)\n",
    "\n",
    "np.save('old-narrative-queries_embeddings.npy', query_embeddings)\n",
    "\n",
    "query_search_index = AnnoyIndex(query_embeddings.shape[1], 'manhattan')\n",
    "\n",
    "for index, embed_value in enumerate(query_embeddings):\n",
    "    query_search_index.add_item(index, embed_value)\n",
    "\n",
    "query_search_index.build(n_trees = 20, n_jobs = 3)\n",
    "\n",
    "query_search_index.save('old-narrative-queries_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can either use the CPU or GPU to encode (use gpu if possible as its faster), but don't use both at the same time so as not to do the computation twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding narratives: CPU version\n",
    "narratives_embedding = model.encode(sentences=tokenized_narratives_df[\"lemmatized_selftext\"], \n",
    "                                    convert_to_numpy=True, \n",
    "                                    show_progress_bar=True)\n",
    "\n",
    "np.save('RS_2018-01_narrative_embeddings.npy', narratives_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding narratives: GPU version\n",
    "pool = model.start_multi_process_pool()\n",
    "\n",
    "narratives_embedding = model.encode_multi_process(sentences=tokenized_narratives_df[\"lemmatized_selftext\"], \n",
    "                                                pool=pool)\n",
    "\n",
    "model.stop_multi_process_pool(pool=pool)\n",
    "\n",
    "np.save('RS_2018-01_narrative_embeddings.npy', narratives_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "\n",
    "for index_embedding, embed_value in enumerate(tqdm(narratives_embedding)):\n",
    "    narratives_search_index.add_item(index_embedding, embed_value)\n",
    "\n",
    "narratives_search_index.build(n_trees = 20, n_jobs = -1)\n",
    "\n",
    "narratives_search_index.save(f'RS_2018-01_narrative_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load embeddings and search indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and search indexes for new narratives \n",
    "narratives_embedding = np.load('RS_2018-01_narrative_embeddings.npy')\n",
    "narratives_search_index = AnnoyIndex(np.array(narratives_embedding).shape[1], 'manhattan')\n",
    "narratives_search_index.load('RS_2018-01_narrative_search_index.ann')\n",
    "\n",
    "# oldnarrative based: load embeddings and search indexes queries\n",
    "query_embeddings = np.load('old-narrative-queries_embeddings.npy')\n",
    "query_search_index = AnnoyIndex(np.array(query_embeddings).shape[1], 'manhattan')\n",
    "query_search_index.load('old-narrative-queries_search_index.ann')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Concatenating (for a single query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive nearest neighbors\n",
    "results_list = []\n",
    "\n",
    "for index, query_embedding in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(query_embedding, n=100, include_distances=True)\n",
    "\n",
    "    result_df = pd.DataFrame(data={\n",
    "        'selftext': narrative_df['selftext'][similar_item_ids[0]],\n",
    "        'title': narrative_df['title'][similar_item_ids[0]],\n",
    "        'ids': narrative_df['id'][similar_item_ids[0]],\n",
    "        'distance': similar_item_ids[1]\n",
    "    })\n",
    "\n",
    "    results_list.append(result_df)\n",
    "\n",
    "results = pd.concat(results_list)\n",
    "results = results.drop_duplicates(subset=['selftext'])\n",
    "results = results.sort_values(by=['distance'], ascending=True)\n",
    "\n",
    "results.to_csv(\"results_tokenized_full.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparative Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(id_to_distance_dictionary: dict, threshold: float) -> float:\n",
    "    \"\"\"\n",
    "    Filters for stories which contain intersecting topics specified by the threshold. \n",
    "\n",
    "    :param id_to_distance_dictionary\n",
    "    :param\n",
    "\n",
    "    return: \n",
    "        tuple containing all the ids for relevant narratives and their respective distances\n",
    "    \"\"\"\n",
    "   \n",
    "    filtered_id_distance_array = []\n",
    "\n",
    "    for comparable_key in id_to_distance_dictionary:\n",
    "        # array we are currently trying to filter\n",
    "        comparable_id_array = id_to_distance_dictionary[comparable_key][0]\n",
    "        comparable_distance_array = id_to_distance_dictionary[comparable_key][1]\n",
    "\n",
    "        for comparable_id_index, comparable_id in enumerate(comparable_id_array):\n",
    "            percentage = 0\n",
    "            for tocompare_key in id_to_distance_dictionary:\n",
    "                if tocompare_key != comparable_key:\n",
    "                    tocompare_id_array = id_to_distance_dictionary[tocompare_key][0]\n",
    "\n",
    "                    if tocompare_id_array.__contains__(comparable_id): \n",
    "                        percentage += (1 / (len(id_to_distance_dictionary)-1))\n",
    "\n",
    "            if percentage >= threshold: \n",
    "                filtered_id_distance_array.append((comparable_id, comparable_distance_array[comparable_id_index],))\n",
    "\n",
    "    return filtered_id_distance_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_dict = {}\n",
    "for index, query_embedding in tqdm(enumerate(query_embeddings)): \n",
    "    similar_item_ids = narratives_search_index.get_nns_by_vector(\n",
    "                                    query_embedding,\n",
    "                                    n=1000, \n",
    "                                    include_distances=True)\n",
    "\n",
    "    similar_dict[f\"query_{index}\"] = similar_item_ids\n",
    "\n",
    "\n",
    "filterd_arr= compare(similar_dict, threshold= 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_narratives_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, similar_item_ids in enumerate(filterd_arr): \n",
    "\n",
    "    new_row = {\n",
    "        'selftext': tokenized_narratives_df['selftext'][similar_item_ids[0]],\n",
    "        'title': tokenized_narratives_df['title'][similar_item_ids[0]],\n",
    "        'subreddit': tokenized_narratives_df['subreddit'][similar_item_ids[0]],\n",
    "        'permalink': tokenized_narratives_df['permalink'][similar_item_ids[0]],\n",
    "        'ids': tokenized_narratives_df['id'][similar_item_ids[0]],\n",
    "        'distance': similar_item_ids[1]\n",
    "    }\n",
    "\n",
    "    if index != 0:\n",
    "        filtered_results_df.loc[len(filtered_results_df)] = new_row\n",
    "    else: \n",
    "        filtered_results_df = pd.DataFrame([new_row])\n",
    "\n",
    "filtered_results_df.to_csv(\"Filtered_Results_RS_2018-01.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty row to the bottom of the dataframe\n",
    "import pandas as pd \n",
    "results_df = pd.read_csv(\"Filtered_Results_RS_2018-01.csv\")\n",
    "results_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[results_df[\"distance\"] < 20.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.document_writer import docx_writer\n",
    "from paths import HOME\n",
    "\n",
    "docx_writer(num_people=5, dataframe= results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
