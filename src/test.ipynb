{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import base64 \n",
    "import zstandard as zstd\n",
    "from paths import COMPRESSED_SUMBISSIONS, DECOMPRESSED_SUMBISSIONS\n",
    "\n",
    "from pathlib import Path\n",
    "from os.path import isfile, join as pjoin\n",
    "\n",
    "from json_load_write import load_json_data\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZStandardDcmp = zstd.ZstdDecompressor()\n",
    "\n",
    "# compressedData = base64.b64decode()\n",
    "# stream_reader = ZStandardDcmp.stream_reader(compressedData)\n",
    "# decompressedTXTData = stream_reader.read().decode('utf-8-sig')\n",
    "# stream_reader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pjoin(COMPRESSED_SUMBISSIONS, \"things\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json(json_array):\n",
    "    combined_json = {}\n",
    "    for obj in json_array:\n",
    "        combined_json.update(obj)\n",
    "    pprint(combined_json)\n",
    "        \n",
    "    return combined_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compressed_files = os.listdir(COMPRESSED_SUMBISSIONS)\n",
    "dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n",
    "for file in compressed_files: \n",
    "    if file.__contains__('2018'): \n",
    "        print(file)\n",
    "        file_rb = open(pjoin(COMPRESSED_SUMBISSIONS, file), 'rb')        \n",
    "        stream_reader = dctx.stream_reader(file_rb)\n",
    "        f   = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        \n",
    "        json_array = []\n",
    "        for index, l in enumerate(f): \n",
    "            json_array.append(json.loads(l))\n",
    "\n",
    "        write_data(f\"{file.split('.')[0]}.json\", json_array)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompressed_files = os.listdir(DECOMPRESSED_SUMBISSIONS)\n",
    "for file_name in os.listdir(DECOMPRESSED_SUMBISSIONS): \n",
    "        print(file_name)\n",
    "        file_name_no_ext = file_name.split(\".\")[0]\n",
    "        # file_as_json = load_json_data()\n",
    "\n",
    "        df = pd.concat(pd.read_json(pjoin(DECOMPRESSED_SUMBISSIONS, file_name), chunksize=1000, lines=True))\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msgspec.json import decode\n",
    "from msgspec import Struct\n",
    "\n",
    "class id(Struct):\n",
    "    id: str\n",
    "\n",
    "class url(Struct):\n",
    "    url: str\n",
    "\n",
    "class permalink(Struct):\n",
    "    permalink: str\n",
    "\n",
    "class subreddit(Struct):\n",
    "    subreddit: str\n",
    "\n",
    "class title(Struct):\n",
    "    title: str\n",
    "\n",
    "class selftext(Struct):\n",
    "    selftext: str\n",
    "\n",
    "class Interaction(Struct):\n",
    "    id: str\n",
    "    selftext: str\n",
    "    title: str\n",
    "    subreddit: str \n",
    "    permalink: str\n",
    "    url: str\n",
    "for file_name in os.listdir(DECOMPRESSED_SUMBISSIONS): \n",
    "    with open(pjoin(DECOMPRESSED_SUMBISSIONS, file_name), \"rb\") as f:\n",
    "        data = decode(f.read(), type=list[Interaction])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100000\n",
    "\n",
    "# Step 3: Initialize an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Step 4: Iterate over the JSON file in chunks\n",
    "for file_name in os.listdir(DECOMPRESSED_SUMBISSIONS): \n",
    "    with open(pjoin(DECOMPRESSED_SUMBISSIONS, file_name), 'r') as file:\n",
    "        for chunk in pd.read_json(file, lines=True, chunksize=chunk_size):\n",
    "            df = pd.concat([df, chunk])\n",
    "            break\n",
    "\n",
    "    df.head()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100000\n",
    "\n",
    "for file_name in os.listdir(DECOMPRESSED_SUMBISSIONS): \n",
    "    df = pd.DataFrame()\n",
    "    df_chunk = pd.read_json(pjoin(DECOMPRESSED_SUMBISSIONS, file_name), lines=True, chunksize=chunk_size)\n",
    "    for chunk in df_chunk:\n",
    "        print(\"thing\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100000\n",
    "for file_name in os.listdir(DECOMPRESSED_SUMBISSIONS): \n",
    "    if not os.path.exists(pjoin(DECOMPRESSED_SUMBISSIONS, file_name.split('.')[0] + \".csv\")):\n",
    "        df = pd.DataFrame()\n",
    "        df_chunk = pd.read_json(pjoin(DECOMPRESSED_SUMBISSIONS, file_name), lines=True, chunksize=chunk_size)\n",
    "        \n",
    "        for index, chunk in enumerate(df_chunk):\n",
    "            chunk = chunk[[\"id\", \"selftext\", \"title\", \"subreddit\", \"permalink\", \"url\"]]\n",
    "            chunk = chunk.dropna()\n",
    "            chunk = chunk.drop_duplicates(subset=[\"id\"])\n",
    "            chunk = chunk[chunk['selftext'].str.len() > 100]\n",
    "            chunk[\"lemmatized_selftext\"] = chunk[\"selftext\"].apply(lemmatize)\n",
    "            \n",
    "            df = pd.concat([df, chunk])\n",
    "            # print every 10th chunk\n",
    "            if (index + 1) % 10 == 0:\n",
    "                print(f\"Current size of df: {df.shape[0]} and at chunk {index + 1}\")\n",
    "\n",
    "        # if file_name.csv not in data folder, create it\n",
    "        \n",
    "            df.to_csv(pjoin(DECOMPRESSED_SUMBISSIONS, file_name.split('.')[0] + \".csv\"))\n",
    "        break\n",
    "    else: \n",
    "        print(f\"File {file_name.split('.')[0]}.csv already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(pjoin(DECOMPRESSED_SUMBISSIONS, file_name.split('.')[0] + \".csv\")):\n",
    "    print(\"thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(pjoin(DECOMPRESSED_SUMBISSIONS, file_name.split('.')[0] + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_search import * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpR_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
